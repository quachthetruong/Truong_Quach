[{"content":"For most developers, QuickSort is a fast and efficient sorting algorithm with a time complexity of O(nlogn). This makes it significantly better than other common sorting algorithms, like Selection Sort or Bubble Sort, which have a time complexity of O(n²). However, the question remains: Why is the average time complexity of QuickSort O(nlogn)?\nIn this blog, we will delve deep into the mathematical and probabilistic principles that explain this efficiency, helping you understand the underlying reasons why QuickSort is faster than other algorithms on average.\nQuickSort Basics: A Reminder Before diving into the mathematical reasoning, let’s quickly remind ourselves how QuickSort works. QuickSort is a divide-and-conquer sorting algorithm that recursively partitions an array into two subsets: one with elements smaller than a pivot value and the other with elements larger than the pivot value. This partitioning continues until the array is fully sorted.\nHere\u0026rsquo;s the code that implements the partitioning process in QuickSort:\n# Partition function def partition(arr, low, high): pivot = arr[high] i = low - 1 for j in range(low, high): if arr[j] \u0026lt; pivot: i += 1 swap(arr, i, j) swap(arr, i + 1, high) return i + 1 # Swap function def swap(arr, i, j): arr[i], arr[j] = arr[j], arr[i] # The QuickSort function implementation def quickSort(arr, low, high): if low \u0026lt; high: pi = partition(arr, low, high) quickSort(arr, low, pi - 1) quickSort(arr, pi + 1, high) How Can an Algorithm Be Considered Effective? An algorithm is considered effective if it solves a problem efficiently, especially with the least number of comparisons.\nFor example, consider the following JavaScript code that sorts an array of words based on their length:\nconst words = [\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;cherry\u0026#34;, \u0026#34;kiwi\u0026#34;, \u0026#34;pear\u0026#34;]; words.sort((a, b) =\u0026gt; a.length - b.length); In this case, the number of comparisons refers to how many times the expression a.length - b.length is evaluated. The function compares the length of each word and orders them accordingly. The question we need to answer is: How many comparisons does this algorithm make?\nAn effective sorting algorithm minimizes unnecessary comparisons. For instance, if we’ve already compared two elements (e.g., \u0026ldquo;apple\u0026rdquo; and \u0026ldquo;kiwi\u0026rdquo;), there’s no need to compare them again unless it\u0026rsquo;s necessary. This reduces the total number of comparisons, thus improving the algorithm’s efficiency.\nOptimal sorting algorithms handle this efficiently by avoiding redundant checks. For example, if we know that \u0026ldquo;kiwi\u0026rdquo; is already longer than \u0026ldquo;apple\u0026rdquo; and \u0026ldquo;banana\u0026rdquo; is shorter, there’s no need to directly compare \u0026ldquo;kiwi\u0026rdquo; with \u0026ldquo;banana\u0026rdquo; again.\nWhat is Average Time Complexity? To understand average time complexity, we need to review some fundamental concepts from probability and statistics that every Vietnamese student learns in their first year of university. If you\u0026rsquo;re unfamiliar with it, you can take a few minutes to revisit this concept on Expected Value.\nLet’s break down the concept with an example:\nWe start with an ordered array:\noriginal = [1, 3, 4, 5, 6, 8, 9]; After shuffling the elements:\nshuffled = [3, 6, 5, 1, 4, 8, 9]; Now, we define a random variable 𝑋𝑖𝑗 that equals 1 if the algorithm does compare the i-th smallest and j-th smallest elements in the original array, and 0 if it does not. Let 𝑋 denote the total number of comparisons made by the algorithm. Since the algorithm never compares the same pair of elements twice, we have:\nTherefore, the expected value of 𝑋, denoted E[𝑋], is:\nUnderstanding E[𝑋𝑖𝑗] Now, let’s consider one of these 𝑋𝑖𝑗’s for i \u0026lt; j. Denote the i-th smallest element in the array by e𝑖and the j-th smallest element by e𝑗. Conceptually, imagine lining up the elements in sorted order. There are three possible cases for the pivot selection during QuickSort:\nCase 1: The pivot is between e𝑖 and 𝑒𝑗 In this case, the two elements e𝑖 and 𝑒𝑗end up in different partitions, and we will never compare them. This is because the pivot has separated these two elements into separate subsets.\nCase 2: The pivot is exactly e𝑖 or 𝑒𝑗 If the pivot chosen during the partitioning step is either e𝑖 or 𝑒𝑗, then we will compare these two elements directly because they are now in the same subset.\nCase 3: The pivot is less than e𝑖 or greater than 𝑒𝑗 You might wonder: What happens if the pivot is less than e𝑖 or greater than 𝑒𝑗? In these situations, the pivot does not directly affect the comparison between e𝑖 and 𝑒𝑗. Once the partitioning step occurs, both e𝑖 and 𝑒𝑗 will still end up in the same subset. Ultimately, they will converge into one of the two scenarios above where they are compared directly, and thus this case does not contribute to the expectation.\nAt each step, the probability that 𝑋𝑖𝑗 = 1 (i.e., we compare e𝑖 and 𝑒𝑗) is exactly 2/(j−i+1). Therefore, the overall probability that 𝑋𝑖𝑗 = 1 is:\nSumming Up the Expected Value This means that for a given element i, it is compared to element i+1 with probability 1, to element i+2 with probability 2/3, to element i+3 with probability 2/4, and so on. Therefore, the expected value of X is:\nThe sum of the series 1 + 1/2 + 1/3 + ... + 1/n, denoted 𝐻𝑛, is called the nth harmonic number. This series grows logarithmically and can be approximated as:\nln is the natural logarithm and γ is the Euler-Mascheroni constant, approximately 0.577. Since γ is a constant, it does not affect the overall growth rate of the sum. Therefore, in Big-O notation, we can express the growth of 𝐻𝑛 as O(lnn), meaning that as n increases,the harmonic number grows logarithmically.\nThus, we can bound the expected value of 𝑋 as:\nConclusion Through this blog, we’ve seen how QuickSort’s average complexity of O(nlogn) arises from the expected value, driven by the harmonic series. While QuickSort is often used as an example, many sorting algorithms can also be understood probabilistically, just like this. Whether or not you see this as essential knowledge, it’s an interesting and indispensable topic in understanding algorithm efficiency at a deeper level. The formulas and concepts we used may seem abstract, but they’re a powerful tool for analyzing and optimizing algorithms.\nReferences: https://www.cs.cmu.edu/afs/cs/academic/class/15451-s07/www/lecture_notes/lect0123.pdf https://en.wikipedia.org/wiki/Harmonic_series_(mathematics) https://en.wikipedia.org/wiki/Expected_value https://www.geeksforgeeks.org/quick-sort-algorithm/ ","permalink":"https://quachthetruong.github.io/posts/technical/quick-sort-time-complexity/","summary":"\u003cp\u003eFor most developers, QuickSort is a fast and efficient sorting algorithm with a time complexity of \u003ccode\u003eO(nlogn)\u003c/code\u003e. This makes it significantly better than other common sorting algorithms, like Selection Sort or Bubble Sort, which have a time complexity of \u003ccode\u003eO(n²)\u003c/code\u003e. However, the question remains: \u003cstrong\u003eWhy is the average time complexity of QuickSort O(nlogn)?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIn this blog, we will delve deep into the mathematical and probabilistic principles that explain this efficiency, helping you understand the underlying reasons why QuickSort is faster than other algorithms on average.\u003c/p\u003e","title":"Why the Average Complexity of QuickSort is O(nlogn)?"},{"content":"Part 1: Motivation How do we check if something is in a set — fast? The simplest way is a List:\nif x in items: ... But this is O(n) — too slow for large-scale systems.\nA HashSet improves to O(1) lookups on average, but it stores the full elements, requiring more memory than raw data — especially for strings or objects.\nSo what if we trade a little accuracy for massive savings? What if a structure could:\nUse only ~9.6 bits per element (significantly smaller than storing object or string),\nBe wrong only 1% of the time (false positives),\nAnd never say \u0026ldquo;no\u0026rdquo; to something that’s truly there?\nThat’s the power of the Bloom Filter.\nWhere is it used? Bloom filters are quietly at the heart of many systems:\nLSM trees, the foundation of modern NoSQL databases like Apache Cassandra, MongoDB, use Bloom filters to skip disk reads — asking: “Does this file probably contain the key?”\nPlatforms like Quora, Medium, and Yahoo use Bloom filters to:\nPrevent duplicate content,\nAvoid redundant processing,\nSpeed up internal caching systems.\nEven if you don’t see them — Bloom filters are working behind the scenes, making large systems fast and efficient.\nPart 2: What is a false positive? (Definition + Example) What is a false positive? ✅ When you check an element that was inserted, the Bloom filter says “yes” — that’s a true positive, and it’s 100% guaranteed correct.\n⚠️ But sometimes it says “yes” to something that was never inserted — that’s a false positive.\nA false positive happens when a new element matches the bit pattern of others — even though it was never added.\nThe filter sees all bits set and says “Probably yes” — but it’s wrong.\nThat’s the trade-off for speed and space — and you’ll see it clearly in the next example.\nHow does it work? A Bloom filter is built with:\nm: a bit array of length m, all bits start at 0\nk: k independent hash functions\nn: number of elements inserted\nTo insert an element O(1):\nHash it using all k functions\nFlip the corresponding k bits to 1\nTo check membership O(1):\nHash the element using the same k functions\nIf any of the k bits is 0 → the element is definitely not in the set\nIf all are 1 → the element is possibly in the set (could be a false positive)\nExample: When a False Positive Happens Let’s step through a small Bloom filter:\nSetup: m = 10 (bit array of size 10)\nk = 3 hash functions\nInserted elements: \u0026lsquo;apple\u0026rsquo; and \u0026lsquo;banana\u0026rsquo; so n = 2\nStart with an empty bit array:\nInitial: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] Step 1: Insert \u0026lsquo;apple\u0026rsquo; Let’s say:\nh₁(apple) = 2\nh₂(apple) = 4\nh₃(apple) = 7\nUpdate the bit array:\nAfter insert apple: [0, 0, 1, 0, 1, 0, 0, 1, 0, 0] Step 2: Insert \u0026lsquo;banana\u0026rsquo; Let’s say:\nh₁(banana) = 1\nh₂(banana) = 4\nh₃(banana) = 8\nUpdate the bit array:\nAfter insert banana: [0, 1, 1, 0, 1, 0, 0, 1, 1, 0] Step 3: Check \u0026lsquo;banana\u0026rsquo; (True Positive) h₁(banana) = 1 → bit is 1\nh₂(banana) = 4 → bit is 1\nh₃(banana) = 8 → bit is 1\nAll bits are 1 → Bloom filter says “Yes” → ✅ correct!\nStep 4: Check \u0026lsquo;mango\u0026rsquo; (False Positive) h₁(mango) = 2\nh₂(mango) = 4\nh₃(mango) = 7\nCheck bits:\n2 → 1 ✅\n4 → 1 ✅\n7 → 1 ✅\nBloom filter says “Yes”, but \u0026lsquo;mango\u0026rsquo; was never inserted → ❌ false positive\nThis happens because \u0026lsquo;apple\u0026rsquo; and \u0026lsquo;banana\u0026rsquo; already set those bits.\nPart 3: Mathematical Proof Okay — now you understand what a false positive is. Let’s take it a step deeper and explore the probability theory behind Bloom filters.\nDon’t worry — it only uses basic math that every student learns in university. We\u0026rsquo;ll walk through this step-by-step, keeping things intuitive.\nWhat is our goal? We want to answer:\nWhat’s the probability that a new element (not in the set) returns a false positive?\nThat means: all the k bits checked during the query are already 1— even though the element was never inserted, not even once among the n inserted items.\nStep 1: Probability a Bit is Still 0 After One Flip Suppose we have an array of m bits, all starting as 0. Now we flip 1 random bit to 1. The chance that a specific bit stays 0 is:\nWhy? Because we only had a 1/m chance to hit it.\nStep 2: We Perform k × n Bit Flips We insert n elements, each hashed with k functions. So we flip bits k × n times.\nThe probability that a specific bit is still 0 after all those flips is:\nStep 3: Exponential Limit Approximation When m is large and kn is not too huge, we can approximate this with the exponential function:\nThis comes from the identity:\nStep 4: Probability Bit is 1 (i.e. Flipped at Least Once) This tells us how likely a bit is to be 1 after inserting n elements.\nStep 5: False Positive = All k Bits Are 1 Now, suppose we query a new element that wasn’t inserted. Its k hash functions give us k bit positions. The probability that all those k bits are already 1 — just by chance — is:\nAnd that’s the final formula for Bloom filter false positives.\nBloom Filter Example: 1 Million Items (n = 1,000,000) Size per element (m/n) Total Size Hash Functions (k) False Positive Rate 6.25 bits 0.78 MB 4 4.99% 7.5 bits 0.94 MB 5 2.70% 9.58 bits 1.20 MB 6 1.00% 12.5 bits 1.56 MB 8 0.25% Part 4: What’s Next — Other Smart Probabilistic Structures for Big Data Problems Bloom filters solve set membership efficiently — but what about other fundamental questions in computer science?\nHow many unique users have viewed this video? → HyperLogLog\nHow many times did user X access this page? → Count-Min Sketch\nWhat are the 50th, 90th, and 99th percentiles of the measured latencies? → t-digest\nWant more? → Explore more probabilistic data structures.\nReferences: https://www.amazon.com/Probabilistic-Data-Structures-Algorithms-Applications/dp/3748190484 https://en.wikipedia.org/wiki/Category:Probabilistic_data_structures https://brilliant.org/wiki/bloom-filter/ https://redis.io/docs/latest/develop/data-types/probabilistic/ https://en.wikipedia.org/wiki/Bloom_filter ","permalink":"https://quachthetruong.github.io/posts/technical/bloom-filters-explained/","summary":"\u003ch2 id=\"part-1-motivation\"\u003ePart 1: Motivation\u003c/h2\u003e\n\u003ch3 id=\"how-do-we-check-if-something-is-in-a-set--fast\"\u003eHow do we check if something is in a set — fast?\u003c/h3\u003e\n\u003cp\u003eThe simplest way is a \u003ccode\u003eList\u003c/code\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e x \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e items:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#f92672\"\u003e...\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eBut this is \u003ccode\u003eO(n)\u003c/code\u003e — too slow for large-scale systems.\u003c/p\u003e\n\u003cp\u003eA \u003ccode\u003eHashSet\u003c/code\u003e improves to \u003ccode\u003eO(1)\u003c/code\u003e lookups on average,\nbut it stores the full elements, requiring \u003cstrong\u003emore memory than raw data\u003c/strong\u003e — especially for strings or objects.\u003c/p\u003e\n\u003ch3 id=\"so-what-if-we-trade-a-little-accuracy-for-massive-savings\"\u003eSo what if we trade a little accuracy for massive savings?\u003c/h3\u003e\n\u003cp\u003eWhat if a structure could:\u003c/p\u003e","title":"Bloom Filters Explained: A Fast and Space-Efficient Probabilistic Solution"}]